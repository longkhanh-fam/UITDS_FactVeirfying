{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uyWEfJnF_8SD",
        "2eSeNKrG7U3-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wSBvyuhq1CyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py_vncorenlp"
      ],
      "metadata": {
        "id": "fiOgYMRFkSCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --single-branch --branch fast_tokenizers_BARTpho_PhoBERT_BERTweet https://github.com/datquocnguyen/transformers.git\n",
        "%cd transformers\n",
        "!pip3 install --upgrade .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "_V3W0vFNUoQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "PBkXzcpwUjGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "Pp_7qqOKXR7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning-transformers"
      ],
      "metadata": {
        "id": "AE_6zTjVN-c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate transformers"
      ],
      "metadata": {
        "id": "Wy0eEN3mr9Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PhoBert/BARTpho"
      ],
      "metadata": {
        "id": "78F207OaN0GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datasets import load_dataset\n",
        "from torch import nn\n",
        "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "I0bdDmw2iwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import py_vncorenlp\n",
        "\n",
        "# Automatically download VnCoreNLP components from the original repository\n",
        "# and save them in some local machine folder\n",
        "py_vncorenlp.download_model(save_dir='./')"
      ],
      "metadata": {
        "id": "LH1msULWkjIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the word and sentence segmentation component\n",
        "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./')\n",
        "\n",
        "text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n",
        "\n",
        "output = rdrsegmenter.word_segment(text)\n",
        "\n",
        "print(output)\n",
        "# ['Ông Nguyễn_Khắc_Chúc đang làm_việc tại Đại_học Quốc_gia Hà_Nội .', 'Bà Lan , vợ ông Chúc , cũng làm_việc tại đây .']"
      ],
      "metadata": {
        "id": "cpC_p60JbVH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
        "sentence = 'Chúng_tôi là những nghiên_cứu_viên .'\n",
        "\n",
        "input_ids = torch.tensor([tokenizer.encode(sentence, padding = True, truncation=True)])\n",
        "\n",
        "with torch.no_grad():\n",
        "    features = phobert(input_ids)  # Models outputs are now tuples\n",
        "\n",
        "features"
      ],
      "metadata": {
        "id": "93wbejcWbp90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maplabel(ver):\n",
        "  if ver == 'SUPPORTED':\n",
        "    return 0\n",
        "  elif ver == 'REFUTED':\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "JnLX5AdXbqIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maptarget(ver):\n",
        "  if ver == 0:\n",
        "    return 'SUPPORTED'\n",
        "  elif ver == 1:\n",
        "    return 'REFUTED'\n",
        "  else:\n",
        "    return 'NEI'"
      ],
      "metadata": {
        "id": "hPgTaaCqmxGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PrepareData"
      ],
      "metadata": {
        "id": "4MSOFphW_rIz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH43NgnjSZnx"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/DataUITDSC/ise-dsc01-train.json', 'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data:\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "id": "d8k28xlvjZ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc 5.000 gia Hà Nội.\\nBà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n",
        "\n",
        "output = rdrsegmenter.word_segment(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "JP6MeqjyD6fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(rdrsegmenter.word_segment(data['7125']['context']))"
      ],
      "metadata": {
        "id": "IYDuZhvEmsM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafr = {'key': [], 'context': [], 'claim': [], 'label': [], 'evidence': []}\n",
        "for key in tqdm(data):\n",
        "  datafr['key'].append(key)\n",
        "  datafr['context'].append(\"\".join(rdrsegmenter.word_segment(data[key]['context'])))\n",
        "  datafr['claim'].append(rdrsegmenter.word_segment(data[key]['claim'])[0].replace('\\x00', ''))\n",
        "  datafr['label'].append(maplabel(data[key]['verdict']))\n",
        "  datafr['evidence'].append(data[key]['evidence'])\n"
      ],
      "metadata": {
        "id": "APq2ZOAHuZ8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(datafr)\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "j2HLS-PABkQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.fillna(' ')"
      ],
      "metadata": {
        "id": "HN9ZYtvHnYZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "ailAQrZpnzT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/dict_guid/DSC2023training/train.csv', index=False)"
      ],
      "metadata": {
        "id": "o1XBUikIGEb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff =  pd.read_csv('/content/drive/MyDrive/dict_guid/DSC2023training/train.csv')\n",
        "len(dff)"
      ],
      "metadata": {
        "id": "GMuma0HHHKCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PrepareDataVal"
      ],
      "metadata": {
        "id": "uyWEfJnF_8SD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjkS4b2P_8SJ"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/DataUITDSC/Bản sao của ise-dsc01-warmup.json', 'r') as file:\n",
        "  data_val = json.load(file)\n",
        "\n",
        "len(data_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_val:\n",
        "  print(data_val[i])\n",
        "  break"
      ],
      "metadata": {
        "id": "8TalW3rD_8SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_val_fr = {'key': [], 'context': [], 'claim': [], 'label': [], 'evidence': []}\n",
        "for key in tqdm(data_val):\n",
        "  data_val_fr['key'].append(key)\n",
        "  data_val_fr['context'].append(\"\".join(rdrsegmenter.word_segment(data_val[key]['context'])))\n",
        "  data_val_fr['claim'].append(rdrsegmenter.word_segment(data_val[key]['claim'])[0].replace('\\x00', ''))\n",
        "  data_val_fr['label'].append(maplabel(data_val[key]['verdict']))\n",
        "  data_val_fr['evidence'].append(data_val[key]['evidence'])\n"
      ],
      "metadata": {
        "id": "wBKTX8kF_8SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.DataFrame(data_val_fr)\n",
        "\n",
        "val_df.head()"
      ],
      "metadata": {
        "id": "HpgWBefU_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = val_df.fillna(' ')"
      ],
      "metadata": {
        "id": "-oQUK4tD_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head()"
      ],
      "metadata": {
        "id": "dMCFd0Mc_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.to_csv('/content/drive/MyDrive/dict_guid/DSC2023training/val.csv', index=False)"
      ],
      "metadata": {
        "id": "DIeYZOFe_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff_val =  pd.read_csv('/content/drive/MyDrive/dict_guid/DSC2023training/val.csv')\n",
        "len(dff_val)"
      ],
      "metadata": {
        "id": "D6HP-adj_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count2 = 0\n",
        "for sample in val_df.values:\n",
        "  if sample[3] == 0:\n",
        "    count2 += 1\n",
        "print(count2)"
      ],
      "metadata": {
        "id": "Ft8zh1h3_8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PrepareData3\n"
      ],
      "metadata": {
        "id": "27Bi6GDB_yKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex = ' Ban tổ chức và Ban giám khảo sẽ chấm điểm, chọn bài thi vào vòng chung kết dựa trên 40% bình chọn của độc giả và 60% điểm Ban giám khảo'\n",
        "if ex[0] == ' ':\n",
        "  ex1 = ex[1:]\n",
        "print(ex)\n",
        "print(ex1)\n"
      ],
      "metadata": {
        "id": "kbR860igzgUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Gueorguiev được phép trò chuyện với mọi người 30 phút mỗi ngày.\"Tôi muốn thử thách bản thân\", Gueorguiev nói'.split('.\"')"
      ],
      "metadata": {
        "id": "ni2-a3Dp5Jem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in data['46609']['context'].split('\\n'):\n",
        "      if not j:\n",
        "        continue\n",
        "      if j[0] == ' ':\n",
        "        j = j[1:]\n",
        "      j = j.replace('\\n', '')\n",
        "      print(j)"
      ],
      "metadata": {
        "id": "4CRC5oyO3GQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['46609']['evidence'])"
      ],
      "metadata": {
        "id": "pDCUzpk13w_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "count = 0\n",
        "datafr_excep = {'key': [], 'context': [], 'claim': [], 'label': [], 'evidence': []}\n",
        "evidence_df = {'key': [], 'sentence': [], 'claim': [], 'label': []}\n",
        "for key in tqdm(data):\n",
        "  if data[key]['verdict'] != \"NEI\":\n",
        "    check = True\n",
        "    for k in re.split('\\. |\\.\\n|\\.\\s|\\n', data[key]['context']):\n",
        "      j = k\n",
        "      if not j:\n",
        "        continue\n",
        "      if j[0] == ' ':\n",
        "        j = j[1:]\n",
        "      j = j.replace('\\n', '')\n",
        "      j = j.replace('. ', '')\n",
        "      rd = np.random.randint(0, 8, 1)[0]\n",
        "      if (j == data[key]['evidence'] or j+'.' == data[key]['evidence']):\n",
        "        # print('split')\n",
        "        # print(j)\n",
        "        # print(data[key]['evidence'])\n",
        "        s = j.replace('\\x00', '')\n",
        "        c = data[key]['claim'].replace('\\x00', '')\n",
        "        evidence_df['key'].append(key)\n",
        "        evidence_df['sentence'].append(\"\".join(rdrsegmenter.word_segment(s)))\n",
        "        evidence_df['claim'].append(\"\".join(rdrsegmenter.word_segment(c)))\n",
        "        evidence_df['label'].append(1)\n",
        "        count += 1\n",
        "        check = False\n",
        "      elif rd == 1:\n",
        "        s = j.replace('\\x00', '')\n",
        "        c = data[key]['claim'].replace('\\x00', '')\n",
        "        evidence_df['key'].append(key)\n",
        "        evidence_df['sentence'].append(\"\".join(rdrsegmenter.word_segment(s)))\n",
        "        evidence_df['claim'].append(\"\".join(rdrsegmenter.word_segment(c)))\n",
        "        evidence_df['label'].append(0)\n",
        "\n",
        "    if check:\n",
        "      datafr_excep['key'].append(key)\n",
        "      datafr_excep['context'].append(data[key]['context'])\n",
        "      datafr_excep['claim'].append(data[key]['claim'])\n",
        "      datafr_excep['label'].append(data[key]['verdict'])\n",
        "      datafr_excep['evidence'].append(data[key]['evidence'])\n"
      ],
      "metadata": {
        "id": "eHg_BL-uvdPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in evidence_df['label']:\n",
        "  if i == 1:\n",
        "    count += 1\n",
        "\n",
        "print(count)\n"
      ],
      "metadata": {
        "id": "iAQnxGUcU2GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train_df = pd.DataFrame(evidence_df)"
      ],
      "metadata": {
        "id": "tnpaqP4B3lfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "evidence_train, evidence_test = train_test_split(evidence_train_df, test_size=0.1, random_state=128)"
      ],
      "metadata": {
        "id": "iZgtVTax7hAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train.head()"
      ],
      "metadata": {
        "id": "8uIwYB8-3g51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_test.head()"
      ],
      "metadata": {
        "id": "7chjBsM28Jek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train.to_csv('/content/drive/MyDrive/dict_guid/DSC2023evidence/evidence_train.csv', index=False)\n",
        "evidence_test.to_csv('/content/drive/MyDrive/dict_guid/DSC2023evidence/evidence_test.csv', index=False)"
      ],
      "metadata": {
        "id": "s-gHFlAb3UTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(evidence_train.values)"
      ],
      "metadata": {
        "id": "lqZVKE3Y4lp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[586, 'claim'] = s"
      ],
      "metadata": {
        "id": "Wjp-2RqlaSti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.values[586][2]"
      ],
      "metadata": {
        "id": "9X2LQJW_eAHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('/content/drive/MyDrive/dict_guid/DSC2023/test.csv', index=False)"
      ],
      "metadata": {
        "id": "bw4wpsCsaeDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FactClasify"
      ],
      "metadata": {
        "id": "2eSeNKrG7U3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"train.csv\", \"validation\": \"val.csv\"}\n",
        "train_dataset = load_dataset(\"/content/drive/MyDrive/dict_guid/DSC2023training\", split=\"train\")\n",
        "val_dataset = load_dataset(\"/content/drive/MyDrive/dict_guid/DSC2023training\", split=\"validation\")"
      ],
      "metadata": {
        "id": "MqEICjL6CFX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "ba556WeMffeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(batched_text):\n",
        "    a = tokenizer(batched_text['context'], padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "    c = {'context_input_ids': a['input_ids'],'context_attention_mask': a['attention_mask']}\n",
        "    return c\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(tokenization, batched = True, batch_size = len(train_dataset))\n",
        "val_dataset = val_dataset.map(tokenization, batched = True, batch_size = len(val_dataset))"
      ],
      "metadata": {
        "id": "T3iOtHNlfC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "q6qFxH7wgAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization2(batched_text):\n",
        "    a = tokenizer(batched_text['claim'], padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "    c = {'claim_input_ids': a['input_ids'],'claim_attention_mask': a['attention_mask']}\n",
        "    return c\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(tokenization2, batched = True, batch_size = len(train_dataset))\n",
        "val_dataset = val_dataset.map(tokenization2, batched = True, batch_size = len(val_dataset))"
      ],
      "metadata": {
        "id": "ID7sUOBXhhLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format('torch', columns=['context_input_ids', 'context_attention_mask', 'claim_input_ids', 'claim_attention_mask', 'label'])\n",
        "val_dataset.set_format('torch', columns=['context_input_ids', 'context_attention_mask', 'claim_input_ids', 'claim_attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "I6DtFCUq_L8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "a6Ys_cLiidph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClaimClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ClaimClassifier, self).__init__()\n",
        "        self.bert_context = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "        self.bert_claim = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.bert_claim.config.hidden_size, self.bert_claim.config.hidden_size),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(self.bert_claim.config.hidden_size, n_classes)\n",
        "                                 )\n",
        "\n",
        "\n",
        "    def forward(self, context_input_ids, context_attention_mask, claim_input_ids, claim_attention_mask):\n",
        "        last_hidden_state_context, output_context = self.bert_context(\n",
        "            input_ids=context_input_ids,\n",
        "            attention_mask=context_attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        last_hidden_state_claim, output_claim = self.bert_claim(\n",
        "            input_ids=claim_input_ids,\n",
        "            attention_mask=claim_attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        output = output_context*output_claim\n",
        "\n",
        "        x = self.drop(output)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eHFPXkedO_xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "dtouYXvhjJjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "f9svWz4uAvXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# Recommendation by BERT: lr: 5e-5, 2e-5, 3e-5\n",
        "# Batchsize: 16, 32\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer,\n",
        "#             num_warmup_steps=0,\n",
        "#             num_training_steps=len(train_loader)*EPOCHS\n",
        "#         )\n",
        "best_acc = 0\n",
        "for epoch in range(100):\n",
        "    print(f'Epoch {24+epoch+1}/{2}')\n",
        "    print('-'*30)\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for data in tqdm(train_loader):\n",
        "        context_input_ids = torch.tensor(data['context_input_ids']).to(device)\n",
        "        context_attention_mask = torch.tensor(data['context_attention_mask']).to(device)\n",
        "        claim_input_ids = torch.tensor(data['claim_input_ids']).to(device)\n",
        "        claim_attention_mask = torch.tensor(data['claim_attention_mask']).to(device)\n",
        "        targets = torch.tensor(data['label'], dtype=torch.long).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            context_input_ids=context_input_ids,\n",
        "            context_attention_mask=context_attention_mask,\n",
        "            claim_input_ids=claim_input_ids,\n",
        "            claim_attention_mask=claim_attention_mask\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct += torch.sum(pred == targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "    print(f'Train Accuracy: {correct.double()/len(train_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "    if not (24+epoch+1)%2:\n",
        "      stri = f'/content/drive/MyDrive/dict_guid/DSC2023model/factchecking_new_{24+epoch+1}.pth'\n",
        "      torch.save(model.state_dict(), stri)\n"
      ],
      "metadata": {
        "id": "ma7yGTAul3gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "lAhn3qpMdsFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dff.values:\n",
        "  print(data[3])"
      ],
      "metadata": {
        "id": "vFy56S1PlRW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "for data in tqdm(valid_loader):\n",
        "        context_input_ids = torch.tensor(data['context_input_ids']).to(device)\n",
        "        context_attention_mask = torch.tensor(data['context_attention_mask']).to(device)\n",
        "        claim_input_ids = torch.tensor(data['claim_input_ids']).to(device)\n",
        "        claim_attention_mask = torch.tensor(data['claim_attention_mask']).to(device)\n",
        "        targets = torch.tensor(data['label'], dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            context_input_ids=context_input_ids,\n",
        "            context_attention_mask=context_attention_mask,\n",
        "            claim_input_ids=claim_input_ids,\n",
        "            claim_attention_mask=claim_attention_mask\n",
        "        )\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(pred == targets)\n",
        "\n",
        "        # print(maptarget(pred), maptarget(targets))\n",
        "print(f'Train Accuracy: {correct.double()/len(valid_loader.dataset)}')\n"
      ],
      "metadata": {
        "id": "yOV0ediWikku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EvidenceExtract"
      ],
      "metadata": {
        "id": "lkCvp9vl804l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"evidence_train.csv\", \"test\": \"evidence_test.csv\"}\n",
        "evidence_train_dataset = load_dataset(\"/content/drive/MyDrive/dict_guid/DSC2023evidence\", split=\"train\")\n",
        "evidence_val_dataset = load_dataset(\"/content/drive/MyDrive/dict_guid/DSC2023evidence\", split=\"test\")"
      ],
      "metadata": {
        "id": "9nbr4pwM85it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in evidence_train_dataset:\n",
        "  if i['label'] == 1:\n",
        "    count += 1\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "id": "KBLNj-k9EKRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(evidence_train_dataset))"
      ],
      "metadata": {
        "id": "SkBD2CEeE-Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(batched_text):\n",
        "    a = tokenizer(batched_text['sentence'], padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "    c = {'sentence_input_ids': a['input_ids'],'sentence_attention_mask': a['attention_mask']}\n",
        "    return c\n",
        "\n",
        "\n",
        "\n",
        "evidence_train_dataset = evidence_train_dataset.map(tokenization, batched = True, batch_size = len(evidence_train_dataset))\n",
        "evidence_val_dataset = evidence_val_dataset.map(tokenization, batched = True, batch_size = len(evidence_val_dataset))"
      ],
      "metadata": {
        "id": "UZJ838CH9N3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization2(batched_text):\n",
        "    a = tokenizer(batched_text['claim'], padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "    c = {'claim_input_ids': a['input_ids'],'claim_attention_mask': a['attention_mask']}\n",
        "    return c\n",
        "\n",
        "\n",
        "\n",
        "evidence_train_dataset = evidence_train_dataset.map(tokenization2, batched = True, batch_size = len(evidence_train_dataset))\n",
        "evidence_val_dataset = evidence_val_dataset.map(tokenization2, batched = True, batch_size = len(evidence_val_dataset))"
      ],
      "metadata": {
        "id": "dXNeDwxo9cKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train_dataset"
      ],
      "metadata": {
        "id": "_JkuXFng-BRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train_dataset.set_format('torch', columns=['sentence_input_ids', 'sentence_attention_mask', 'claim_input_ids', 'claim_attention_mask', 'label'])\n",
        "evidence_val_dataset.set_format('torch', columns=['sentence_input_ids', 'sentence_attention_mask', 'claim_input_ids', 'claim_attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "dXkWSiae-MB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_train_loader = DataLoader(evidence_train_dataset, batch_size=32, shuffle=True)\n",
        "evidence_valid_loader = DataLoader(evidence_val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tNqmHuDL-bkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class evidenceClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(evidenceClassifier, self).__init__()\n",
        "        self.bert_sentence = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "        self.bert_claim = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.bert_claim.config.hidden_size, self.bert_claim.config.hidden_size),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(self.bert_claim.config.hidden_size, n_classes)\n",
        "                                 )\n",
        "\n",
        "    def forward(self, sentence_input_ids, sentence_attention_mask, claim_input_ids, claim_attention_mask):\n",
        "        last_hidden_state_sentence, output_sentence = self.bert_sentence(\n",
        "            input_ids=sentence_input_ids,\n",
        "            attention_mask=sentence_attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        last_hidden_state_claim, output_claim = self.bert_claim(\n",
        "            input_ids=claim_input_ids,\n",
        "            attention_mask=claim_attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        output = output_sentence*output_claim\n",
        "\n",
        "        x = self.drop(output)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "88zW7V41-xLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Py-D7STY_EFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_evidence = evidenceClassifier(2).to(device)\n",
        "model_evidence.load_state_dict(torch.load('/content/drive/MyDrive/dict_guid/DSC2023model/evidencechecking_new_31.pth'))"
      ],
      "metadata": {
        "id": "qGpFZS4pdQRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# Recommendation by BERT: lr: 5e-5, 2e-5, 3e-5\n",
        "# Batchsize: 16, 32\n",
        "optimizer = AdamW(model_evidence.parameters(), lr=2e-5)\n",
        "\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(\n",
        "#             optimizer,\n",
        "#             num_warmup_steps=0,\n",
        "#             num_training_steps=len(evidence_train_loader)*EPOCHS\n",
        "#         )\n",
        "best_acc = 0\n",
        "for epoch in range(100):\n",
        "    print(f'Epoch {1+epoch+1}/{100}')\n",
        "    print('-'*30)\n",
        "\n",
        "    model_evidence.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for data in tqdm(evidence_train_loader):\n",
        "        sentence_input_ids = torch.tensor(data['sentence_input_ids']).to(device)\n",
        "        sentence_attention_mask = torch.tensor(data['sentence_attention_mask']).to(device)\n",
        "        claim_input_ids = torch.tensor(data['claim_input_ids']).to(device)\n",
        "        claim_attention_mask = torch.tensor(data['claim_attention_mask']).to(device)\n",
        "        targets = torch.tensor(data['label'], dtype=torch.long).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_evidence(\n",
        "            sentence_input_ids=sentence_input_ids,\n",
        "            sentence_attention_mask=sentence_attention_mask,\n",
        "            claim_input_ids=claim_input_ids,\n",
        "            claim_attention_mask=claim_attention_mask\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct += torch.sum(pred == targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model_evidence.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "    print(f'Train Accuracy: {correct.double()/len(evidence_train_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "    torch.save(model_evidence.state_dict(), f'/content/drive/MyDrive/dict_guid/DSC2023model/evidencechecking_new_{1+epoch+1}.pth')\n"
      ],
      "metadata": {
        "id": "flV868JJ_PIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Infer"
      ],
      "metadata": {
        "id": "5NaBx2KQtAKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "g7NJnvaEh4LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/DataUITDSC/ise-dsc01-public-test-offcial.json', 'r') as file:\n",
        "  data_test = json.load(file)\n",
        "\n",
        "len(data_test)"
      ],
      "metadata": {
        "id": "jBeAmkUA5X9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/DataUITDSC/ise-dsc01-public-test-offcial.json', 'r') as file:\n",
        "  data_test_seg = json.load(file)\n",
        "\n",
        "len(data_test_seg)"
      ],
      "metadata": {
        "id": "00z39ZlVf1ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in tqdm(data_test_seg):\n",
        "  data_test_seg[key]['context'] = \"\".join(rdrsegmenter.word_segment(data_test_seg[key]['context']))\n",
        "  data_test_seg[key]['claim'] = \"\".join(rdrsegmenter.word_segment(data_test_seg[key]['claim']))"
      ],
      "metadata": {
        "id": "FCmRkOpvf8K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = data_test_seg\n",
        "        self.list_key = list(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        context = self.data[self.list_key[index]]['context']\n",
        "        claim = self.data[self.list_key[index]]['claim']\n",
        "\n",
        "        return context, claim, self.list_key[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hg5uLlePgMgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset()\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    num_workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "8vQm6RYfgQN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sup_score(stri1, stri2):\n",
        "  num_wo = 0\n",
        "  le = len(stri2.split())\n",
        "  for word in stri2.split():\n",
        "    if word in stri1:\n",
        "      num_wo += 1\n",
        "  return num_wo/le\n"
      ],
      "metadata": {
        "id": "8ISZzpRCcA2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "Dc-YdQ17yXLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(re.split('\\n\\.|\\s\\.|\\n', data_test_seg['41942']['context'])))"
      ],
      "metadata": {
        "id": "r8Aj_nHQjdGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in re.split('\\n\\.|\\s\\.|\\n', data_test_seg['41942']['context']):\n",
        "  print(k)"
      ],
      "metadata": {
        "id": "CDAPfCC5i_0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = {}\n",
        "model.eval()\n",
        "model_evidence.eval()\n",
        "\n",
        "correct = 0\n",
        "for dat in tqdm(loader):\n",
        "        check = True\n",
        "        contexts, claims, keys = dat\n",
        "        a = tokenizer(contexts, padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "        b = tokenizer(claims, padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "\n",
        "        context_input_ids = torch.tensor(a['input_ids']).to(device)\n",
        "        context_attention_mask = torch.tensor(a['attention_mask']).to(device)\n",
        "        claim_input_ids = torch.tensor(b['input_ids']).to(device)\n",
        "        claim_attention_mask = torch.tensor(b['attention_mask']).to(device)\n",
        "        # targets = torch.tensor(data['label'], dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            context_input_ids=context_input_ids,\n",
        "            context_attention_mask=context_attention_mask,\n",
        "            claim_input_ids=claim_input_ids,\n",
        "            claim_attention_mask=claim_attention_mask\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        # correct += torch.sum(pred == targets)\n",
        "        for idx, key_id in enumerate(keys):\n",
        "          if preds[idx] != 2:\n",
        "            ss = []\n",
        "            cc = []\n",
        "            for k in re.split('\\. |\\.\\n|\\.\\s|\\n', data_test[key_id]['context']):\n",
        "              sen = tokenizer(\"\".join(rdrsegmenter.word_segment(k)), padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "              cla = tokenizer(contexts[idx], padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "              print(contexts[idx])\n",
        "              sentence_input_ids = torch.tensor([sen['input_ids']]).to(device)\n",
        "              sentence_attention_mask = torch.tensor([sen['attention_mask']]).to(device)\n",
        "              claim_input_ids = torch.tensor([b['input_ids']]).to(device)\n",
        "              claim_attention_mask = torch.tensor([b['attention_mask']]).to(device)\n",
        "              print(sentence_input_ids.shape)\n",
        "              print(sentence_attention_mask.shape)\n",
        "              print(claim_input_ids.shape)\n",
        "              print(claim_attention_mask.shape)\n",
        "\n",
        "              out_evi = model_evidence(\n",
        "                  sentence_input_ids=sentence_input_ids,\n",
        "                  sentence_attention_mask=sentence_attention_mask,\n",
        "                  claim_input_ids=claim_input_ids,\n",
        "                  claim_attention_mask=claim_attention_mask\n",
        "              )\n",
        "\n",
        "              sof = nn.Softmax()\n",
        "              out_prob = sof(out_evi)\n",
        "\n",
        "              sup = sup_score(k, data_test[key_id]['claim'])\n",
        "              # print(out_prob)\n",
        "              # break\n",
        "\n",
        "              score = out_prob[0][1].item()\n",
        "\n",
        "              score = 0.3*score + 0.7*sup\n",
        "\n",
        "              ss.append(k)\n",
        "              cc.append(score)\n",
        "            # break\n",
        "            idx = torch.argmax(torch.tensor(cc))\n",
        "            evi = ss[idx]\n",
        "            print(cc[idx])\n",
        "            print(evi)\n",
        "            print(data_test[key_id]['claim'])\n",
        "            ans[key_id] = {\"context\": data_test[key_id]['context'],\n",
        "                  \"claim\": data_test[key_id]['claim'],\n",
        "                  \"verdict\": maptarget(preds[idx]),\n",
        "                  \"evidence\": evi}\n",
        "            check = False\n",
        "              # if preds[idx]_evi == 1:\n",
        "              #   evi = k\n",
        "              #   print(evi)\n",
        "              #   print(data_test[key_id]['claim'])\n",
        "              #   ans[key_id] = {\"context\": data_test[key_id]['context'],\n",
        "              #         \"claim\": data_test[key_id]['claim'],\n",
        "              #         \"verdict\": maptarget(preds[idx]),\n",
        "              #         \"evidence\": evi}\n",
        "              #   check = False\n",
        "              #   break\n",
        "          if check:\n",
        "            ans[key_id] = {\"context\": data_test[key_id]['context'],\n",
        "                        \"claim\": data_test[key_id]['claim'],\n",
        "                        \"verdict\": maptarget(preds[idx]),\n",
        "                        \"evidence\": \"\"}\n",
        "          print(maptarget(preds[idx]))\n",
        "        # print(maptarget(preds[idx]), maptarget(targets))\n",
        "# print(f'Train Accuracy: {correct.double()/len(valid_loader.dataset)}')\n"
      ],
      "metadata": {
        "id": "jPDf03YXgYJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = {}\n",
        "model.eval()\n",
        "correct = 0\n",
        "for key in tqdm(data_test):\n",
        "        check = True\n",
        "        a = tokenizer(\"\".join(rdrsegmenter.word_segment(data_test[key]['context'])), padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "        b = tokenizer(\"\".join(rdrsegmenter.word_segment(data_test[key]['claim'])), padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "\n",
        "        context_input_ids = torch.tensor([a['input_ids']]).to(device)\n",
        "        context_attention_mask = torch.tensor([a['attention_mask']]).to(device)\n",
        "        claim_input_ids = torch.tensor([b['input_ids']]).to(device)\n",
        "        claim_attention_mask = torch.tensor([b['attention_mask']]).to(device)\n",
        "        # targets = torch.tensor(data['label'], dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            context_input_ids=context_input_ids,\n",
        "            context_attention_mask=context_attention_mask,\n",
        "            claim_input_ids=claim_input_ids,\n",
        "            claim_attention_mask=claim_attention_mask\n",
        "        )\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "        # correct += torch.sum(pred == targets)\n",
        "        if pred != 2:\n",
        "          ss = []\n",
        "          cc = []\n",
        "          for k in re.split('\\. |\\.\\n|\\.\\s|\\n', data_test[key]['context']):\n",
        "            sen = tokenizer(\"\".join(rdrsegmenter.word_segment(k)), padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "            cla = tokenizer(\"\".join(rdrsegmenter.word_segment(data_test[key]['claim'])), padding = True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "\n",
        "            sentence_input_ids = torch.tensor([sen['input_ids']]).to(device)\n",
        "            sentence_attention_mask = torch.tensor([sen['attention_mask']]).to(device)\n",
        "            claim_input_ids = torch.tensor([b['input_ids']]).to(device)\n",
        "            claim_attention_mask = torch.tensor([b['attention_mask']]).to(device)\n",
        "\n",
        "\n",
        "            out_evi = model_evidence(\n",
        "                sentence_input_ids=sentence_input_ids,\n",
        "                sentence_attention_mask=sentence_attention_mask,\n",
        "                claim_input_ids=claim_input_ids,\n",
        "                claim_attention_mask=claim_attention_mask\n",
        "            )\n",
        "\n",
        "            sof = nn.Softmax()\n",
        "            out_prob = sof(out_evi)\n",
        "\n",
        "            sup = sup_score(k, data_test[key]['claim'])\n",
        "            # print(out_prob)\n",
        "            # break\n",
        "\n",
        "            score = out_prob[0][1].item()\n",
        "\n",
        "            score = 0.3*score + 0.7*sup\n",
        "\n",
        "            ss.append(k)\n",
        "            cc.append(score)\n",
        "          # break\n",
        "          idx = torch.argmax(torch.tensor(cc))\n",
        "          evi = ss[idx]\n",
        "          print(cc[idx])\n",
        "          print(evi)\n",
        "          print(data_test[key]['claim'])\n",
        "          ans[key] = {\"context\": data_test[key]['context'],\n",
        "                \"claim\": data_test[key]['claim'],\n",
        "                \"verdict\": maptarget(pred),\n",
        "                \"evidence\": evi}\n",
        "          check = False\n",
        "            # if pred_evi == 1:\n",
        "            #   evi = k\n",
        "            #   print(evi)\n",
        "            #   print(data_test[key]['claim'])\n",
        "            #   ans[key] = {\"context\": data_test[key]['context'],\n",
        "            #         \"claim\": data_test[key]['claim'],\n",
        "            #         \"verdict\": maptarget(pred),\n",
        "            #         \"evidence\": evi}\n",
        "            #   check = False\n",
        "            #   break\n",
        "        if check:\n",
        "          ans[key] = {\"context\": data_test[key]['context'],\n",
        "                      \"claim\": data_test[key]['claim'],\n",
        "                      \"verdict\": maptarget(pred),\n",
        "                      \"evidence\": \"\"}\n",
        "        print(maptarget(pred))\n",
        "        # print(maptarget(pred), maptarget(targets))\n",
        "# print(f'Train Accuracy: {correct.double()/len(valid_loader.dataset)}')\n"
      ],
      "metadata": {
        "id": "Kjyc4Yp_Mdcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ans))"
      ],
      "metadata": {
        "id": "Hnbd2AZ5zYvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json_obj = json.dumps(ans)\n",
        "with open(\"/content/drive/MyDrive/dict_guid/ResultDSC/ans2810_2.json\", \"w\", encoding='utf-8') as outfile:\n",
        "    # outfile.write(json_obj)\n",
        "    json.dump(ans, outfile, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "So4NkDHN_qba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}